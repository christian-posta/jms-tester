---
title: JMSTester User's Guide
--- name:overview pipeline:tags,textile
h1. {title:}

The place to go for questions about {project_name:}

--- name:content pipeline:tags,textile
As you are reading this introduction you have probably downloaded the JMS Test Framework. Why have you done that?
There might be a number of reasons:
  
* You want to learn about JMS and went searching for some sample code that does more than sending and receiving a "Hello World" message.
* You want to benchmark your particular JMS Provider and the usual benchmarking suspects seemed to complicated or haven't done the 
  job for you. 
* You want to fine tune the performance of your JMS layer specifically to the throughput you have to support and make sure 
  you have selected a scalable architecture. 

In all cases you have come to the right place. The JMS Test Framework has grown over several years - starting it's life on a 
JMS consultant's laptop as a single stand alone JMS application that would send as many messages to a JMS broker as possible 
and also consume them as fast as possible with some minor statistics like msg/s produced, msg/s consumed etc. 
  
Before we actually get started with testing your JMS provider, let's have a quick view what the JMS Testframework is designed 
to do and which are current (non-)goals.

h2. {project_name:} features

  
If you consider the simple approach described above, there are several drawbacks with that approach, that have been discovered 
and addressed over time:
  
* First of all, think of the characteristics of that performance 
  test. It only says, that a particular JMS layer with one client connected on a particular hardware can support x msg/s. That might 
  be o.k. to get a "feeling" of the capabilities of the JMS layer, but with a test framework we want to replace "feeling" by "knowing".
  Therefore the test framework has been redesigned to be run in a distributed fashion: Run as many instances of the benchmarking 
  client as needed to produce a sufficient load and have them coordinated by a single benchmark controller. All the configuration 
  is done where the controller lives (to be more specific, where the controller front-end lives ...). 
* Even if you can multiple clients and coordinate them from a central place, this is not quite what is needed in a practical 
  project oriented benchmarking scenario. Normally the JMS layer needs to support 
  a given number of messages per time unit (i.e. 5000 msg/s). Perhaps you have to cope with peaks and see if the messaging layer can 
  cope with those as well. To achieve this kind of test, the test framework allows the definition of load profiles. A load profile 
  is simply a function over time. This function gives the desired producer message rate per second for the duration of the test. A load 
  profile is also always limited with respect to time. A simple load profile might be: generate a message load from 500 to 1000 msg/s 
  stepping 100 msg/s every 10s. The entire test will run 60s, generating 500, 600, 700, 800, 900, 1000 msg/s for 10s each.
  <br/>  
  The load profiles can be combined and can have loops, so that it is very easy to simulate a realistic load of messages. 
* If you think about sending messages, you won't always think about simple text messages, but also of other messaging types as 
  defined in the JMS specification. Therefore the test framework has a pluggable mechanism to generate the messages used in the 
  benchmark. The default mechanism produces text messages of a defined length, but that is easy to replace or extend. 
* Once you are able to run as many producers and consumers to inject the desired load into the messaging layer, you will need 
  statistics. The standard statistics you are interested in are the number of messages that have been produced and consumed by 
  each individual client for the duration of the test and also some statistics indicating the state of the messaging layer 
  and the hardware you are running on.
  <br/>
  The JMS test framework contains a complete probing framework as an abstraction layer on top of statistic gathering. Each producer 
  and consumer has the standard probes (messaging rate, latency and msg size) enabled per default. In addition the framework has 
  implementations of JMX probes, that allow you to query any JMX numerical attribute and include it as a statistic. Also the JMS test
  framework contains an example of a sigar probe, which allows you to query machine related data like CPU usage and IOStats.
  <br/>
  All Probes adhere to some very simple Java interfaces, so that Probes can be added according to specific project requirements.
  Regardless which probes you use - existing ones or ones to come - all of them are considered in the reporting engine.
* This brings us to an important aspect of the JMS test framework - the reporting. If you have done benchmarks in the past, you probably 
  ended up doing some screenshots and/or copy/paste operations into a spreadsheet processor of your choice to produce some decent graphs. 
  If you run many tests or want to compare a number of different configurations, generating the reports might be tiresome. 
  <br/>
  One of the design goals of the test framework was to make the statistics processing a) pluggable and a core feature of the framework as 
  such. The result of every test run should be a number of files (spreadsheets, graphs, ...) that summarize the outcome of the benchmark. 
  <br/>
  As a result the JMS test framework contains a complete post processing engine that is called upon whenever a benchmark has finished. 
  That engine will record the raw benchmarking data throughout the execution of the benchmark and process the data after the benchmark 
  has finished. Right now we have two post processor implementations:
** A Graphing Post Processor that is based on the RRD4J libraries will spool all recorded data into a RRD database and then generate 
   a simple time based graph for each recorded probe. The graphs will be in PNG format and can be included easily in any of your 
   test documents.
** A CSV Post Processor will spool all recorded data to a CSV file. The first two columns of the file reflect the timestamps of the 
    benchmark run, while the other columns reflect one recorded probe in each column. 
  The post processors have one thing in common: they gather the existence of the different probes dynamically and include them in the reports.
  All you need to do to include an additional element in your reports is to activate another probe. 
* The test framework has been built primarily to benchmark and test ActiveMQ. This having said, you can plug in any JMS provider into the 
  test framework by simply changing the connection factory. We have not used any ActiveMQ specific code within the framework and have made no 
  assumptions about the JMS provider under test. However, ActiveMQ is used by the framework components internally to communicate with each 
  other. In theory you could change that as well, but why should you ;). 

h2. {project_name:} Non-features

There are a number of things that were not in our minds when we put the JMS test framework together. Some of the items may appear voluntarily, 
some might actually make sense to you ...

* The JMS test framework is not a monitoring solution. Though we have the probing framework and the auto generating graphs, you should not 
  consider it to be a monitoring solution. A monitoring solution would provide you with a nice user interface, up to date graphics rather than 
  graphs generated during post processing, a sophisticated user / role concept and alerting mechanisms in case something goes wrong. Typical 
  examples for monitoring solutions are FUSE HQ and Nagios.
* The JMS test framework is an application simulator. One of the design goals of the framework was to provide an easy way to simulate the 
  messaging behavior of your application. While the framework allows you to simulate a rather complex scheme of messaging events, it takes 
  away the complexity of the things attached to the messaging layer. This should help you to decide whether you are looking at a problem in 
  the JMS layer or in your application. If it is a problem of the JMS layer, you can provide the benchmark configuration to your JMS vendor 
  and the problem should be reproducable without relying on your application code.
* For now the JMS test framework does not support content validation tests. That would be a test where you drop a message into a given 
  destination and expect the output of one or more target destinations. The original idea was to support hose kind of tests as well in the sense 
  of blackbox testing a JMS driven application, but we have excluded that for now.
* The JMS test framework is not foolproof. While te framework is very flexible it also gives you a lot of options to run tests that don't
  make any sense. Therefore we recommend that you have a test plan and at least a week of time dedicated to performance testing and tuning 
  before you get started. If you are just in the learning phase of JMS related applications, take your time to run some simple tests, read
  a good JMS book and then come back with a test plan for the real thing. 
* The JMS test framework is for JMS testing only. We don't support other transport protocols than JMS, but we have not put any provider 
  specific code into the framework either. That means you can run the same benchmark setup against a number of different JMS providers if 
  you want to, but you can only test JMS messaging. If we wanted to support more than JMS we would probably go down the route of using Camel 
  and it's protocol abstraction and reuse the probing and statistics framework.

h2. {project_name:} wishlist

Projects like the JMS test framework are never really complete, so here is the current wishlist for volunteers who want to work on the 
project:

* As all projects that are born on a consultant notebook the JMS test framework lacks Documentation. This is true for written documentation 
  and javadoc. Any help on this is more than welcome.
* There are some basic examples for configuring the framework in the tutorials. Additional examples that are derived from real world use 
  cases are highly appreciated - especially when they can be used to demonstrate common pitfalls with a particular JMS provider or JMS messaging 
  in general. 
* It's not meant as an apology, but most of the framework has been written in a plane, onsite and with limited time available. Therefore 
  TestNG unit tests have been neglected throughout the development. Going hand in hand with the javadocs that would be a great thing to spend 
  time with. Can we achieve a decent test coverage ? ;)
* In addition to the current post processors there might be other ones. One user has already asked for a JDBC post-processor, which 
  is very similar to the CSV one. A post processor could also do deeper analysis of the data with respect to deviation analysis and 
  decide whether a certain benchmark shall be considered to be successful or failed. 
* As of today we have a very limited set of front ends - to be exact we have a simple command line interface that can be used to issue 
  commands to the benchmarking clients. It would be kind of straight forward to implement other front-ends (RESTful with some eye candy
  to have something to look at during long running tests or an Eclipse plug-in).
