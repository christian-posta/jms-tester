---
title: JMSTester Probes Tutorial
--- name:overview pipeline:tags,textile
h1. {title:}

Add JMX and OS level probes to your benchmark

--- name:content pipeline:tags,textile

This tutorial walks you though the steps needed to add more probes to your benchmark. Apart from measuring the 
throughput and latency of your JMS layer, sometime you will need an insight into what the JMS broker(s) is doing
or how the OS resources are leveraged. 

To do this, we have introduced a felxible probing framework that can be used with {project_name:}. {project_name:}-core
supports JMX probes. In addition, {project_name:}-sigar supports OS level probes leveraging the *SIGAR* libraries 
by "Hyperic":http://www.hyperic.com/products/sigar.html. 

h2. Prerequesites

Before you can run this tutorial, make sure that you have completed the "installation":{relocatable: ../../install.html}
including the installation of the additional libraries. This is required as the *SIGAR* libraries are distributed 
with a different license, which you must accept before using it. 

h2. Start up the benchmarking framework 

So, you have installed a copy of {project_name:} in *$JMSTESTER_HOME* and are ready to go.

In the directory *$JMSTESTER_HOME* you will find a *bin* sub directory that contains all the necessary scripts to work 
with {project_name:}. The script you need to bring up the framework is called *runBenchmark* (don't worry, we won't 
break anything by executing it - yet;) ). 

So, from $JMSTESTER_HOME execute:

{coderay:: plaintext}
  ./bin/runBenchmark -controller -clientNames TestClient -recorder -spring conf/testScripts
{coderay}
  
You will see the benchmarking framework start -- and do nothing, so what have we actually done? - Well, we have started
a JVM hosting the components of the benchmarking framework.  

* The *clientNames* option is a comma separated list of names to identify benchmarking clients. For each client name given 
  a separate client instance would be started in the JVM. For now you don't need to bother with client names or configurations 
  that accept more than one client - just keep in mind they are there in case you need them. 
* A client does nothing by itself. It needs a *controller* to be run. So we have also started a controller in the JVM using 
  the *controller* option. For it is sufficient to know, the benchmark front-ends will send commands to the controller 
  (for example an execute benchmark command). 
* Now we have a client and a controller instance, but these would not do any reporting. To enable reporting we also need 
  to start the reporting engine using the *recorder* flag. The recorder will then record all data sent from the clients
  in raw format and execute post-processors on the data after the benchmark has finished. You might be asking, which 
  post-processors? - We have defined those with the *spring* flag. This points at a directory containing
  one or more xml-files that are interpreted as spring configuration files. The recorder will execute all post-processors
  that fulfill the BenchmarkPostProcessor interface. 

h2. Execute the benchmark

Now, that the benchmark framework is sitting there doing nothing, we could just as well submit a benchmark, right? Again, from 
$JMSTESTER_HOME execute

{coderay:: plaintext}
  ./bin/runCommand -command submit:conf/testScripts/probes
{coderay}
  
This will start a simple command line client and read in the spring configuration(s) in the given directory. In those 
configurations the command line client will find all BenchmarkConfiguration objects and create a submit benchmark command 
from them. The benchmarks will be submitted to the benchmark controller and there they will be executed one by one. 

h2. Examine the benchmark results

You have probably noticed something happened in the window were you started the benchmark framework after you have 
submitted the benchmark. You will note a number of warnings, which can safely be ignored. They are just telling you
that you have not configured spring objects for the message factory, the destination provider and the connection 
provider. After the framework has created default objects for those, the benchmark is run and you will note log 
messages like 

{coderay:: plaintext}
...BenchmarkIteration [testProfile1] stepping to 500 msg/s for 10 s]
...BenchmarkIteration [testProfile1] stepping to 600 msg/s for 10 s]
...BenchmarkIteration [testProfile1] stepping to 700 msg/s for 10 s]
{coderay}

So it seems that the benchmark runs exactly through the message rates we wanted. We will explain later in this document 
how we achieved this. 

Once the benchmark has finished, you will note log entries like:

{coderay:: plaintext}
... Creating Graph: CONSUMER-TestClient-probes-part1-0-Counter 2009-09-23-12:45:12-2009-09-23-12:45:42
... Creating Graph: CONSUMER-TestClient-probes-part1-0-Latency 2009-09-23-12:45:12-2009-09-23-12:45:42
... Creating Graph: CONSUMER-TestClient-probes-part1-0-MsgSize 2009-09-23-12:45:12-2009-09-23-12:45:42
... Creating Graph: PROBER-TestClient-probes-CpuMonitor 2009-09-23-12:45:12-2009-09-23-12:45:42
... Creating Graph: PROBER-TestClient-probes-localhost_1099-Queue-test1-QueueSize 2009-09-23-12:45:12-2009-09-23-12:45:42
... Creating Graph: PROBER-TestClient-probes-localhost_1099-ThreadCounter 2009-09-23-12:45:12-2009-09-23-12:45:42
... Creating Graph: PRODUCER-TestClient-probes-part1-COUNTER 2009-09-23-12:45:12-2009-09-23-12:45:42
... Written probe Values to : /Users/andreasgies/tmp/jmstester-1.0-SNAPSHOT/probes/benchmark.csv
{coderay}

This is telling you that the benchmark has been finished and that the post-processors for the data have run. One Post-processor 
has produced a graph for each metric that has been gathered throughout the benchmark, while another one has produced 
a csv-file containing all the values that have been gathered for further processing with a spreadsheet processor of your choice.

If you have also worked through the simple tutorial, you will see, that in addition 3 graphs have been created with names starting 
with *PROBER*. These graphs are the JMX and OS level metric graphs.

Have a look at the graphs below to get an impression of the graphs that will be generated. The graph title is constructed
from the client type, the benchmark id, the benchmark part id and the name of the metric. The graph title will also contain 
the timestamps of when the benchmark was started and when the benchmark was finished. (Click on the image for a larger version of the 
image)

{includethumbs: ../../images/probes}

Accordingly, the generated csv file has 3 more columns than in the simple example.

h2. The benchmark configuration

After you have run the example above, you might want to look behind the curtain and study the benchmark configuration that we
have used. So here it is

{coderay:: {lang: xml, line_numbers: true}} 
{include_file: {filename: ../jmstester-core/src/main/resources/testScripts/probes/benchmarks.xml, process_output: true, escape_html: false}}
{coderay}

Let's have a look at the differences of this configuration and the configuration in the "simple Tutorial":{relocatable: TutorialSimple.html}:

# The main difference here is that the benchmark configuration now populates the probeConfigurations attribute. This attribute contains a list 
  of specifications which probes should be run on which host. The default configuration is to run all probes on all clients that are connected 
  to the controller. You can refer to the "distributed Tutorial":{relocatable: TutorialDistributed.html} to see a more complex configuration. 
# From line 58ff. you see 3 bean definitions. These beans are the probes that will be executed with this configuration. If the bean names are 
  not explicitly stated in the probeConfigurations attribute of the benchmark config {project_name:} will resolve those beans by their type.
  All beans implementing *com.fusesource.forge.jmstest.probe.Probe* will be used in that case.
  ** The *ThreadCounter* is a JMX probe simply reporting the number of threads in the JVM the probe is executed upon.  
  ** The *test1Size* probe is an *ActiveMQDestinationProbe*. This class simply inherits from JMXProbe, but uses Active MQ specific 
     attributes to calculate the JMX object name of the MBean to query.
  ** The *CpuMonitor* is an example for a OS level probe. A full list of examples is included in the "distributed Tutorial":{relocatable: TutorialDistributed.html}.
# The *JMXConnector* in line 52 specifies the JMX connection that will be used to execute the JMX probes.  
